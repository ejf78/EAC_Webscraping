}
print(paste0("Done with ",h))
}
results_batch1
# run another one
hosp2 <- search_site (hosp_url_list[2])
# run another one
hosp2 <- search_site (hosp_url_list[2], url_search_terms, search_terms)
sitemap = hosp_url_list[2]
content_search_terms <- search_terms
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
search_urls
search_urls
##########3
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,urls[grep(t, urls, ignore.case = T)])
}
keep_urls
for(t in url_search_terms){
keep_urls <- c(keep_urls,urls[grep(t, search_urls, ignore.case = T)])
}
keep_urls
url_search_terms
##########3
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
keep_urls
# search site function
search_site <- function(sitemap, url_search_terms,
content_search_terms){
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
content_match <- matrix(nrow = length(keep_urls), ncol = 4)
for(p in 1:length(keep_urls)){
page_scrape <- read_html(keep_urls[p]) %>%
html_text2() %>%
str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
df <- content_match %>% as.data.frame()
names(df) <- c("sitemap","url","match","page text")
df
}
# run another one
hosp2 <- search_site(hosp_url_list[2], url_search_terms, search_terms)
page_scrape
# run another one
hosp2 <- search_site(hosp_url_list[1], url_search_terms, search_terms)
hosp2
# run another one
hosp2 <- search_site(hosp_url_list[3], url_search_terms, search_terms)
hosp2
hosp2 %>% filter(!is.na(match))
hosp2
# run another one
hosp2 <- search_site(hosp_url_list[2], url_search_terms, search_terms)
sitemap<- hosp_url_list[2]
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
search_urls
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
keep_urls
content_match <- matrix(nrow = length(keep_urls), ncol = 4)
for(p in 1:length(keep_urls)){
page_scrape <- read_html(keep_urls[p]) %>%
html_text2() %>%
str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
for(p in 1:length(keep_urls)){
print(p)
page_scrape <- read_html(keep_urls[p]) %>%
html_text2() %>%
str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
keep_urls[3]
page_scrape
page_scrape <- read_html(keep_urls[3])
page_scrape
page_scrape %>% html_text2()
keep_urls
grep(".jpeg", keep_urls)
grep(".jpg", keep_urls)
grepl(".jpg", keep_urls)
!grepl(".jpg", keep_urls)
keep_urls[!grepl(".jpg", keep_urls)]
# search site function
search_site <- function(sitemap, url_search_terms,
content_search_terms){
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
### remove things like jpegs, pngs, etc
keep_urls <- keep_urls[!grepl(".jpg", keep_urls)]
keep_urls <- keep_urls[!grepl(".jpeg", keep_urls)]
keep_urls <- keep_urls[!grepl(".png", keep_urls)]
keep_urls <- keep_urls[!grepl(".bmp", keep_urls)]
keep_urls <- keep_urls[!grepl(".gif", keep_urls)]
content_match <- matrix(nrow = length(keep_urls), ncol = 4)
for(p in 1:length(keep_urls)){
page_scrape <- read_html(keep_urls[p]) %>%
html_text2() %>%
str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
df <- content_match %>% as.data.frame()
names(df) <- c("sitemap","url","match","page text")
df
}
# run another one
hosp2 <- search_site(hosp_url_list[2], url_search_terms, search_terms)
hosp2
results_batch1 <- data.frame()
errored_sitemaps <- c()
for(h in hosp_url_list[1:10]){
result <- NA
result <- try(search_site(h, url_search_terms, search_terms), silent = TRUE)
if(is.na(result)){
errored_sitemaps <- c(errored_sitemaps, h)
} else {
results_batch1 <- rbind(results_batch1, result)
}
print(paste0("Done with ",h))
}
errored_sitemaps
results_batch1
results_batch1$sitemape %>% unique()
results_batch1$sitemap %>% unique()
# run another one
hosp2 <- search_site(hosp_url_list[4], url_search_terms, search_terms)
sitemap<- hosp_url_list[4]
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
keep_urls
content_match <- matrix(nrow = length(keep_urls), ncol = 4)
for(p in 1:length(keep_urls)){
print(p)
page_scrape <- read_html(keep_urls[p]) %>%
html_text2() %>%
str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
page_scrape <- read_html(keep_urls[2])
keep_urls[2]
page_scrape <- read_html(keep_urls[2])
page_scrape <- read_html(keep_urls[2])
keep_urls[2]
possibly(read_html(keep_urls[2]),NA)
x <- possibly(read_html(keep_urls[2]),NA)
x
keep_urls[2] %>% possibly(. %>% read_html())
keep_urls[2] %>% possibly(. %>% read_html(), NA)
keep_urls[2] %>% possibly(. %>% read_html(), NA)
keep_urls[2] %>% possibly(read_html, NA)
pos <- possibly(read_html, NA)
pos <- possibly(read_html, NA)
pos(keep_urls[2])
can_we_read <- possibly(read_html, NA)
possibly_read <- possibly(read_html, NA)
possibly_read(keep_urls[2])
possibly_read <- possibly(read_html %>% html_text2 %>% str_to_lower, NA)
possibly_read <- possibly(read_html %>% html_text2() %>% str_to_lower(), NA)
possibly_read <- possibly(read_html() %>% html_text2() %>% str_to_lower(), NA)
possibly_read <- possibly(read_html, NA)
possibly_parse <- possibly(html_text2, NA)
NA %>% str_to_lower()
# make things possibly
possibly_read <- possibly(read_html, NA)
possibly_parse <- possibly(html_text2, NA)
# search site function
search_site <- function(sitemap, url_search_terms,
content_search_terms){
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
### remove things like jpegs, pngs, etc
keep_urls <- keep_urls[!grepl(".jpg", keep_urls)]
keep_urls <- keep_urls[!grepl(".jpeg", keep_urls)]
keep_urls <- keep_urls[!grepl(".png", keep_urls)]
keep_urls <- keep_urls[!grepl(".bmp", keep_urls)]
keep_urls <- keep_urls[!grepl(".gif", keep_urls)]
content_match <- matrix(nrow = length(keep_urls), ncol = 4)
for(p in 1:length(keep_urls)){
page_scrape <- possibly_read(keep_urls[p])
page_scrape <- possibly_pase(page_scrape) %>% str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
df <- content_match %>% as.data.frame()
names(df) <- c("sitemap","url","match","page text")
df
}
# run another one
hosp2 <- search_site(hosp_url_list[4], url_search_terms, search_terms)
# search site function
search_site <- function(sitemap, url_search_terms,
content_search_terms){
html <- read_html(sitemap)
urls <- html %>% html_elements("loc") %>% html_text2
# are any of these URLs additional sitemaps?
# if the search for "xml" returns nothing,
if(identical(urls[grep("xml", urls, ignore.case = T)], character(0))){
search_urls <- urls
} else {
search_urls <- read_nested_urls(urls)
}
# search for key terms in urls
keep_urls <- c()
for(t in url_search_terms){
keep_urls <- c(keep_urls,search_urls[grep(t, search_urls, ignore.case = T)])
}
### remove things like jpegs, pngs, etc
keep_urls <- keep_urls[!grepl(".jpg", keep_urls)]
keep_urls <- keep_urls[!grepl(".jpeg", keep_urls)]
keep_urls <- keep_urls[!grepl(".png", keep_urls)]
keep_urls <- keep_urls[!grepl(".bmp", keep_urls)]
keep_urls <- keep_urls[!grepl(".gif", keep_urls)]
content_match <- matrix(nrow = length(keep_urls), ncol = 4)
for(p in 1:length(keep_urls)){
page_scrape <- possibly_read(keep_urls[p])
page_scrape <- possibly_parse(page_scrape) %>% str_to_lower()
# remove punctuation
page_scrape <- gsub("[[:punct:]]", " ", page_scrape)
# remove extra spaces
page_scrape <- gsub("  ", " ", page_scrape)
# assign match value
match_value <- ifelse(sum(str_detect(page_scrape, content_search_terms)),"Keyword Match", NA)
# col 1: site
content_match[p,1] <- sitemap
# col 2: url
content_match[p,2] <- keep_urls[p]
# col 3: match value
content_match[p,3] <- match_value
# col 4: page text
content_match[p,4] <- page_scrape
}
df <- content_match %>% as.data.frame()
names(df) <- c("sitemap","url","match","page text")
df
}
# run another one
hosp2 <- search_site(hosp_url_list[4], url_search_terms, search_terms)
hosp2
results_batch1 <- data.frame()
errored_sitemaps <- c()
for(h in hosp_url_list[1:10]){
result <- NA
result <- try(search_site(h, url_search_terms, search_terms), silent = TRUE)
if(is.na(result)){
errored_sitemaps <- c(errored_sitemaps, h)
} else {
results_batch1 <- rbind(results_batch1, result)
}
print(paste0("Done with ",h))
}
errored_sitemaps
results_batch1$sitemap %>% unique()
hosp_url_list[1:10]
result
errored_sitemaps
results_batch1$sitemap %>% unique()
hosp_url_list[1:10]
for(h in hosp_url_list[1:10]){
print(h)
}
results_batch1$sitemap %>% unique()
result
str_detect(result, "Error")
results_batch1 <- data.frame()
errored_sitemaps <- c()
for(h in hosp_url_list[1:10]){
result <- try(search_site(h, url_search_terms, search_terms), silent = TRUE)
if(str_detect(result, "Error")){
errored_sitemaps <- c(errored_sitemaps, h)
} else {
results_batch1 <- rbind(results_batch1, result)
}
print(paste0("Done with ",h))
}
errored_sitemaps
results_batch1$sitemap %>% unique()
results_batch1 %>% filter(!is.na(match))
url <- c(nested_xml())
#### set up functions for nested sitemaps ####
## choosing which to search
nested_xml <- function(){
z <- readline("Which sitemap should we search?")
# assign global true/false
TF <<- readline("Do you want to search another sitemap?")
z
}
nested_xml_wrapper <- function(){
url <- c(nested_xml())
if(TF){
url <- c(url,nested_xml())
}
url
}
### just search all
read_nested_urls <- function(xml_list){
nested_urls <<- c()
for(x in xml_list){
html <- read_html(x)
urls <- html %>% html_elements("loc") %>% html_text2
nested_urls <<- c(nested_urls, urls)
}
nested_urls
}
# make things possible
possibly_read <- possibly(read_html, NA)
possibly_parse <- possibly(html_text2, NA)
F
F
#### set up functions for nested sitemaps ####
## choosing which to search
nested_xml <- function(){
z <- readline("Which sitemap should we search?")
# assign global true/false
TF <<- readline("Do you want to search another sitemap?")
z
}
nested_xml_wrapper <- function(){
url <- c(nested_xml())
if(TF){
url <- c(url,nested_xml())
}
url
}
### just search all
read_nested_urls <- function(xml_list){
nested_urls <<- c()
for(x in xml_list){
html <- read_html(x)
urls <- html %>% html_elements("loc") %>% html_text2
nested_urls <<- c(nested_urls, urls)
}
nested_urls
}
# make things possible
possibly_read <- possibly(read_html, NA)
possibly_parse <- possibly(html_text2, NA)
errored_sitemaps
results_batch1$sitemap %>% unique()
results_batch1 %>% filter(!is.na(match))
results_batch1$sitemap %>% unique()
results_batch1 %>% filter(!is.na(match))
hosp_url_list[1:10]
results_batch1 %>% filter(!is.na(match))
results_batch1$sitemap %>% unique()
# view results
errored_sitemaps
results_batch2 <- data.frame()
for(h in hosp_url_list[11:20]){
result <- try(search_site(h, url_search_terms, search_terms), silent = TRUE)
if(str_detect(result, "Error")){
errored_sitemaps <- c(errored_sitemaps, h)
} else {
results_batch2 <- rbind(results_batch2, result)
}
print(paste0("Done with ",h))
}
results_batch2
results_batch2 %>% filter(!is.na(match))
